{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import torch\n",
    "\n",
    "from voc12 import my_dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from utility.image_util import *\n",
    "from tqdm import tqdm\n",
    "from utility import image_util\n",
    "\n",
    "from misc import imutils\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_network = \"net.resnet50_cam\"\n",
    "cam_root = \"../irn_result/cam/\"\n",
    "rgbd_cam_root = \"../irn_result/rgbd_cam\"\n",
    "val_list = \"voc12/val.txt\"\n",
    "voc12_root = \"../Dataset/VOC2012/\"\n",
    "depth_root = \"../result/depth_img/\"\n",
    "\n",
    "device = \"cuda\"\n",
    "rgbd_cam_weights_name = \"../sess/voc_sess/resnet50_rgbd_cam.pth\"\n",
    "cam_weights_name = \"../sess/voc_sess/resnet50_cam.pth\"\n",
    "scales = (1.0, 0.5, 1.5, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(importlib.import_module(\"net.resnet50_cam\"), 'Net')(rgbd=True)\n",
    "\n",
    "# model.load_state_dict(torch.load(cam_weights_name), strict=True)\n",
    "model.load_state_dict(torch.load(rgbd_cam_weights_name), strict=True)\n",
    "\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = my_dataloader.VOC12ClassificationDataset(val_list, voc12_root= voc12_root)\n",
    "dataset = my_dataloader.VOC12_DepthClassificationDataset(val_list, voc12_root= voc12_root, depth_root=depth_root)\n",
    "\n",
    "\n",
    "data_loader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.0720666e-41  3.0720666e-41  8.0077699e-04 ...  1.8155022e-02\n",
      "  -1.9448768e-02 -7.3257852e-03]\n",
      " [ 5.8058789e-04  1.7402628e-02  8.6773623e-05 ...  1.1792872e-02\n",
      "   1.2564420e-02  1.6304037e-02]\n",
      " [ 2.8246477e-02  2.1779416e-02  1.7464655e-02 ...  1.3903867e-02\n",
      "   1.3176864e-02 -1.4397801e-02]\n",
      " ...\n",
      " [-5.3211331e-01 -7.9355121e-01 -8.1098038e-01 ... -7.0640522e-01\n",
      "   1.4548148e+00 -6.0183007e-01]\n",
      " [ 4.4392157e-01 -4.4496733e-01 -3.7525055e-01 ...  7.7908494e-02\n",
      "  -1.1381263e-01 -7.7612203e-01]\n",
      " [-5.4954249e-01  1.4762527e-01 -1.0724183e+00 ... -1.6475817e+00\n",
      "   1.4762527e-01 -8.4583879e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[15]['img'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for iter, pack in enumerate(tqdm(data_loader)):\n",
    "        img = pack['img'].to(device)\n",
    "        x = model(img)\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
