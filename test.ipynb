{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utility import image_util\n",
    "import os\n",
    "from voc12 import my_dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from misc import imutils\n",
    "import numpy as np\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_list_path = \"voc12/val.txt\"\n",
    "voc12_root = \"../Dataset/VOC2012/\"\n",
    "cam_root = \"../result/val_cam/\"\n",
    "depth_root = \"../result/depth_img/\"\n",
    "\n",
    "cam_network = \"net.resnet50_cam\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = my_dataloader.VOC12_Depth_CropClassificationDatasetMSF(img_name_list_path, voc12_root, cam_root, depth_root, scales = (1.0, 1.5, 0.5, 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(98, 44, 363, 207), (0, 132, 315, 276)]\n",
      "[(24, 11, 91, 51), (0, 33, 79, 69)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yunsu/Desktop/Desktop/SKKU/CVML/Code/voc_cam/misc/imutils.py:177: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return ((orig_size[0]-1)//stride+1, (orig_size[1]-1)//stride+1)\n"
     ]
    }
   ],
   "source": [
    "pack = next(iter(data_loader))\n",
    "\n",
    "crop_boxes = pack['crop_boxes']\n",
    "org_size = pack['size']\n",
    "\n",
    "strided_org_size = imutils.get_strided_size(org_size, 4)\n",
    "\n",
    "crop_boxes = [tuple(int(t.item()) for t in crop_box) for crop_box in crop_boxes]\n",
    "strided_crop_boxes = image_util.resize_bbox_list(crop_boxes, org_size, strided_org_size)\n",
    "print(crop_boxes)\n",
    "print(strided_crop_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = pack['name'][0]\n",
    "msf = pack['msf_img_list']\n",
    "org_size = pack['size']\n",
    "strided_org_size = imutils.get_strided_size(org_size, 4)\n",
    "\n",
    "org_cam = pack['cam'][0].cpu().numpy()\n",
    "org_high_res = pack['high_res'][0].cpu().numpy()\n",
    "crop_labels = pack['crop_labels']\n",
    "crop_boxes = pack['crop_boxes']\n",
    "crop_sizes = pack['crop_sizes']\n",
    "scale_crop_sizes = pack['scale_crop_sizes']\n",
    "\n",
    "crop_boxes = [tuple(int(t.item()) for t in crop_box) for crop_box in crop_boxes]\n",
    "\n",
    "strided_crop_boxes = image_util.resize_bbox_list(crop_boxes, org_size, strided_org_size)\n",
    "\n",
    "cam_list = []\n",
    "highres_cam_list = []\n",
    "\n",
    "key = torch.sum(torch.cat(crop_labels, dim=0), dim=0)\n",
    "key = torch.nonzero(key)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor([163]), tensor([265])], [tensor([144]), tensor([315])]]\n",
      "[[tensor([293]), tensor([477])], [tensor([173]), tensor([378])]]\n",
      "(36, 79)\n",
      "(144, 320)\n"
     ]
    }
   ],
   "source": [
    "for idx, msf_img in enumerate(msf):\n",
    "                \n",
    "    msf_img = msf[idx]\n",
    "    label = crop_labels[idx][0]\n",
    "\n",
    "    scale_crop_size = scale_crop_sizes[idx]\n",
    "    org_crop_size = crop_sizes[idx]\n",
    "\n",
    "    scale_strided_size = imutils.get_strided_size(scale_crop_size, 4)\n",
    "    scale_strided_up_size = imutils.get_strided_up_size(size, 16)\n",
    "\n",
    "    \n",
    "    \n",
    "    valid_cat = torch.nonzero(label)[:, 0]\n",
    "    \n",
    "    \n",
    "    \n",
    "print(crop_sizes)\n",
    "print(scale_crop_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
