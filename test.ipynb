{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utility import image_util\n",
    "import os\n",
    "from voc12 import my_dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from misc import imutils\n",
    "import numpy as np\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_list_path = \"voc12/val.txt\"\n",
    "voc12_root = \"../Dataset/VOC2012/\"\n",
    "cam_root = \"../result/val_cam/\"\n",
    "depth_root = \"../result/depth_img/\"\n",
    "\n",
    "cam_network = \"net.resnet50_cam\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = my_dataloader.VOC12_Depth_CropClassificationDatasetMSF(img_name_list_path, voc12_root, cam_root, depth_root, scales = (1.0, 1.5, 0.5, 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(98, 44, 363, 207), (0, 132, 315, 276)]\n",
      "[(24, 11, 91, 51), (0, 33, 79, 69)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yunsu/Desktop/Desktop/SKKU/CVML/Code/voc_cam/misc/imutils.py:177: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return ((orig_size[0]-1)//stride+1, (orig_size[1]-1)//stride+1)\n"
     ]
    }
   ],
   "source": [
    "pack = next(iter(data_loader))\n",
    "\n",
    "crop_boxes = pack['crop_boxes']\n",
    "org_size = pack['size']\n",
    "\n",
    "strided_org_size = imutils.get_strided_size(org_size, 4)\n",
    "\n",
    "crop_boxes = [tuple(int(t.item()) for t in crop_box) for crop_box in crop_boxes]\n",
    "strided_crop_boxes = image_util.resize_bbox_list(crop_boxes, org_size, strided_org_size)\n",
    "print(crop_boxes)\n",
    "print(strided_crop_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = pack['name'][0]\n",
    "msf = pack['msf_img_list']\n",
    "org_size = pack['size']\n",
    "strided_org_size = imutils.get_strided_size(org_size, 4)\n",
    "\n",
    "org_cam = pack['cam'][0].cpu().numpy()\n",
    "org_high_res = pack['high_res'][0].cpu().numpy()\n",
    "crop_labels = pack['crop_labels']\n",
    "crop_boxes = pack['crop_boxes']\n",
    "crop_boxes = [tuple(int(t.item()) for t in crop_box) for crop_box in crop_boxes]\n",
    "\n",
    "strided_crop_boxes = image_util.resize_bbox_list(crop_boxes, org_size, strided_org_size)\n",
    "\n",
    "cam_list = []\n",
    "highres_cam_list = []\n",
    "\n",
    "key = torch.sum(torch.cat(crop_labels, dim=0), dim=0)\n",
    "key = torch.nonzero(key)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, msf_img in enumerate(msf):\n",
    "                \n",
    "    msf_img = msf[idx]\n",
    "    label = crop_labels[idx][0]\n",
    "\n",
    "    size = (crop_boxes[idx][3] - crop_boxes[idx][1], crop_boxes[idx][2] - crop_boxes[idx][0])\n",
    "\n",
    "    strided_size = imutils.get_strided_size(size, 4)\n",
    "    strided_up_size = imutils.get_strided_up_size(size, 16)\n",
    "\n",
    "    valid_cat = torch.nonzero(label)[:, 0]\n",
    "    \n",
    "    outputs = [model(img[0].to(device)) for img in msf_img]\n",
    "                \n",
    "    strided_cam = torch.sum(torch.stack(\n",
    "    [F.interpolate(torch.unsqueeze(o, 0), strided_size, mode='bilinear', align_corners=False)[0] for o\n",
    "        in outputs]), 0)\n",
    "    \n",
    "    highres_cam = [F.interpolate(torch.unsqueeze(o, 1), strided_up_size,\n",
    "                                mode='bilinear', align_corners=False) for o in outputs]\n",
    "\n",
    "    \n",
    "    highres_cam = torch.sum(torch.stack(highres_cam, 0), 0)[:, 0, :size[0], :size[1]]\n",
    "    \n",
    "    highres_cam = highres_cam[valid_cat]\n",
    "    highres_cam /= F.adaptive_max_pool2d(highres_cam, (1, 1)) + 1e-5\n",
    "    \n",
    "    strided_cam = strided_cam[valid_cat]\n",
    "    strided_cam /= F.adaptive_max_pool2d(strided_cam, (1, 1)) + 1e-5\n",
    "    \n",
    "    strided_cam = strided_cam[0].cpu().numpy()\n",
    "    highres_cam = highres_cam[0].cpu().numpy()\n",
    "\n",
    "    strided_cam = image_util.crop_cam_to_org_cam(strided_cam, strided_crop_boxes[idx], strided_org_size)\n",
    "    highres_cam = image_util.crop_cam_to_org_cam(highres_cam, crop_boxes[idx], org_size)\n",
    "    \n",
    "    cam_list.append(strided_cam)\n",
    "    highres_cam_list.append(highres_cam)\n",
    "\n",
    "cam_stack = np.stack(cam_list)\n",
    "cam_stack = (cam_stack + org_cam) / 2\n",
    "\n",
    "highres_cam_stack = np.stack(highres_cam_list)\n",
    "highres_cam_stack = (highres_cam_stack + org_high_res) / 2\n",
    "\n",
    "cam_stack = torch.from_numpy(cam_stack)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
